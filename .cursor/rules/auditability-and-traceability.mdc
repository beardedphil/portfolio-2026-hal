---
description: Auditability and traceability requirements for all agent work
alwaysApply: true
---

# Auditability (Always On)

For any coding task assigned to an implementation agent, the work must be **fully auditable**.

## Implementation Agent: First Step (MANDATORY - DO THIS FIRST)

**CRITICAL: This is the ABSOLUTE FIRST action when picking up a ticket. Do this BEFORE reading the ticket, BEFORE creating a branch, BEFORE any file edits.**

**FORBIDDEN:** Starting implementation work without first checking for existing artifacts. This step is non-negotiable.

### Step-by-Step Checklist (MUST COMPLETE IN ORDER):

**Step 1: Fetch all existing artifacts**
- Read `.hal/api-base-url` file to get the HAL base URL
- Call HAL API: `POST ${baseUrl}/api/artifacts/get` with `{ ticketId: "<ticket-id>" }`
- If API call fails (404, 500, network error, timeout), note the failure but proceed with caution
- If API call succeeds, you will receive an array of artifacts

**Step 2: Check for QA reports and Human in the Loop failures (CRITICAL)**
- Filter artifacts to find QA reports: `agent_type === "qa"` and title matching `QA report for ticket <ticket-id>`
- **If a QA report exists:**
  - **READ IT IMMEDIATELY** - Do not skip this
  - Check the verdict: Look for "QA RESULT: FAIL" or "QA RESULT: PASS" in the report
  - **Check for Human validation failure:** Look for "Human validation failure" section in the report
  - **If verdict is FAIL:**
    - Read the entire QA report carefully
    - Identify all blocking issues listed
    - Understand what fixes are required
    - Understand why the previous implementation failed
    - **DO NOT repeat the same mistakes**
    - **DO NOT start coding until you understand the failure context**
  - **If Human validation failure section exists:**
    - Read the entire QA report carefully, including the Human validation failure section
    - Identify all issues that caused the Human in the Loop failure
    - Understand what fixes are required based on the failure notes
    - Understand why the previous implementation failed human validation
    - **DO NOT repeat the same mistakes**
    - **DO NOT start coding until you understand the failure context**
- **If no QA report exists:** This may be a fresh ticket (proceed to Step 3)

**Step 3: Review existing implementation artifacts (if any)**
- Look for implementation artifacts (`agent_type === "implementation"`)
- Read these artifacts to understand:
  - **Plan artifact:** What approach was previously taken
  - **Worklog artifact:** What was actually done
  - **Changed Files artifact:** What files were changed
  - **Decisions artifact:** What trade-offs/assumptions were made
  - **Verification artifact:** What issues were identified during verification
  - **PM Review artifact:** What potential failures were identified

**Step 4: Synthesize understanding**
- If the ticket has failed QA before:
  - Document in your Plan artifact that you are addressing previous QA failures
  - List the specific issues you will fix
  - Explain how your approach addresses the previous failures
- If the ticket has failed Human in the Loop validation before:
  - Document in your Plan artifact that you are addressing previous Human in the Loop failures
  - List the specific issues you will fix based on the Human validation failure notes
  - Explain how your approach addresses the previous failures
- If the ticket has been worked before but passed QA:
  - Understand what was done and build on it appropriately
- If this is a fresh ticket:
  - Proceed with normal implementation

**Step 5: Only after completing Steps 1-4, proceed with:**
- Creating/checking out the feature branch (see **change-ownership-and-staging-discipline**)
- Reading the ticket content (if not already reviewed)
- Creating a new Plan artifact (or updating if continuing previous work)

### Why This Matters

- **Tickets often fail QA multiple times** - The same ticket may have been worked by previous agents who made mistakes
- **Tickets often fail Human in the Loop validation** - Even after passing QA, tickets can fail when users test them in the UI
- **QA reports contain critical information** - They tell you exactly what was wrong and what needs to be fixed
- **Human validation failure notes contain critical information** - They tell you exactly what failed in user testing and what needs to be fixed
- **Repeating mistakes wastes time** - Understanding previous failures prevents you from making the same errors
- **Building on previous work** - If work was partially completed, you should understand what was done before

### Warning Signs You Skipped This Step

If you find yourself:
- Starting to code without checking artifacts first
- Creating a Plan artifact without mentioning previous QA failures or Human in the Loop failures
- Implementing features that were already identified as problematic in a QA report
- Making the same mistakes that caused a previous QA failure
- Making the same mistakes that caused a previous Human in the Loop failure

**STOP and go back to Step 1.**

## Standard Prompt Format (Supabase source of truth)

- All new stories/tasks must be **tickets** stored in **Supabase** (source of truth). **Do not create or edit ticket files** under `docs/tickets/` directly; tickets are created via the HAL app (PM agent) or a script that writes to the Supabase `tickets` table. The app uses Supabase-only mode (0065) and does not read from `docs/tickets/*.md`.
- **Kanban is Supabase-backed (Supabase-only mode, 0065).** For any ticket change (create, update body, move column): **update the DB directly via HAL API**. The Kanban UI reflects changes within the polling interval (~10s). Use `POST /api/tickets/move` for column moves. Never update ticket files locally — the app does not read them.
- Use the workspace template for **content structure** when creating a ticket (in-app or via script):
  - `docs/templates/ticket.template.md`
- The ticket in Supabase (and, after sync, in `docs/tickets/<task-id>-<short-title>.md`) is the canonical "prompt" (we do **not** create a `prompt.md` artifact).

## Task ID Convention (Uniqueness)

- Task IDs must be **sequential numbers**, zero-padded to 4 digits (e.g. `0001`, `0002`, ...).
- Task IDs must remain **globally unique** within this workspace; they will be used as keys later.

## Required Artifacts (per task)

**MANDATORY:** All artifacts are stored in **Supabase's `agent_artifacts` table**, not in `docs/audit/` folders. 

**FORBIDDEN:** Creating, writing, or editing files in `docs/audit/` directories. Artifacts must be stored in Supabase only via tool calls.

Use HAL API endpoints (tool calls) to store artifacts:

- Ticket in Supabase (not `docs/tickets/*.md` — Supabase-only mode, 0065)
  - Canonical ticket used to brief the implementation agent (includes acceptance criteria). Fetch using tool call `get_ticket_content`.
- **Plan artifact** (agent_type: `implementation`, title: `Plan for ticket <ticket-id>`)
  - 3–10 bullets: intended approach and file touchpoints.
  - Store via tool call: `insert_implementation_artifact` with `artifactType: "plan"`
- **Worklog artifact** (agent_type: `implementation`, title: `Worklog for ticket <ticket-id>`)
  - Timestamped (rough) notes of what was done, in order.
  - Store via tool call: `insert_implementation_artifact` with `artifactType: "worklog"`
- **Changed Files artifact** (agent_type: `implementation`, title: `Changed Files for ticket <ticket-id>`)
  - List of files created/modified/deleted with 1–2 line purpose each.
  - Store via tool call: `insert_implementation_artifact` with `artifactType: "changed-files"`
- **Decisions artifact** (agent_type: `implementation`, title: `Decisions for ticket <ticket-id>`)
  - Any trade-offs/assumptions + why.
  - Store via tool call: `insert_implementation_artifact` with `artifactType: "decisions"`
- **Verification artifact** (agent_type: `implementation`, title: `Verification for ticket <ticket-id>`)
  - QA verification steps: code review + automated checks (build, lint). No manual UI testing at QA stage — user tests in Human in the Loop after merge.
  - **MANDATORY:** Must include TypeScript compilation check: run `npm run build:hal` (or `tsc -b`) and verify it completes with **zero TypeScript errors**. If TypeScript errors exist, the implementation is **NOT ready** and must be fixed before claiming "ready for QA".
  - If verification involves visuals, include screenshot filenames.
  - Store via tool call: `insert_implementation_artifact` with `artifactType: "verification"`
- **PM Review artifact** (agent_type: `implementation`, title: `PM Review for ticket <ticket-id>`)
  - Likelihood of success (0–100%), potential failures, and how to diagnose them.
  - Store via tool call: `insert_implementation_artifact` with `artifactType: "pm-review"`
- **Git diff artifact** (agent_type: `implementation`, title: `Git diff for ticket <ticket-id>`)
  - Full unified git diff of all changes for this ticket.
  - Generate using: `git diff main...HEAD` (or `git diff main` if on feature branch) to get all changes.
  - If no changes exist or diff is empty, include a message explaining why (e.g., "No changes detected" or "All changes already merged").
  - The diff should be in unified diff format and will be displayed with syntax highlighting in the UI.
  - Store via tool call: `insert_implementation_artifact` with `artifactType: "git-diff"`
- **QA Report artifact** (agent_type: `qa`, title: `QA report for ticket <ticket-id>`)
  - Created by QA agent after code review and verification.
  - Store via tool call: `insert_qa_artifact`

## Artifact Format: Source of Truth (No "Copy From Old Audits")

- The source of truth for artifact **format and required files** is:
  - `.cursor/rules/*.mdc` (workspace rules)
  - `docs/templates/ticket.template.md`
  - `docs/process/ticket-verification-rules.md`
- Do **not** determine artifact format by inspecting prior artifacts in Supabase.
  - You may reference older artifacts only as **examples** after you have already checked the rules/templates.
  - Avoid narration like "Checking an existing artifact for format"; instead confirm you are following the rules/templates above.

## QA (Commit + Push → Human in the Loop)

- An implementation agent must **commit** all work for the ticket and **push** it to the remote before claiming the ticket is "done" or "ready for verification."
- "Ready for verification" additionally requires a **clean + synced repo**:
  - working tree is clean (no uncommitted changes, no untracked files created by the task)
  - branch is synced with remote (not ahead/behind)
- "Ready for verification" must happen on a **feature branch** (not `main`) for ticketed work.
- If any untracked/modified files remain, the agent must either:
  - commit them (if they belong to the ticket), or
  - delete/ignore/revert them (if they are accidental/generated), before claiming "ready."
- The ticket must exist in **Supabase** (Supabase-only mode, 0065):
  - Ticket must exist in Supabase. `docs/tickets/*.md` files are not used by the app.

- "Ready for verification" instructions must be **UI-only**:
  - Do not ask the user/PM to commit, push, or otherwise "finish" the task.
  - If the agent still needs to commit/push or store artifacts, the task is **not** ready.

- The completion message to the user/PM must include:
  - the **feature branch name** (so QA can review and merge), and
  - the exact `git status -sb` output.
- The implementation agent **must add the branch name to the ticket** when claiming "ready for QA" (see ticket template: QA → Branch).
- The implementation agent **must store all 7 required artifacts in Supabase** when marking ready for QA (see "Required Artifacts" above).
- The implementation agent **must move the ticket to QA column** (`col-qa`) via HAL API (`POST /api/tickets/move` with `{ ticketId: "<id>", columnId: "col-qa" }`) when marking ready for QA. **FORBIDDEN:** Claiming "ready" without moving the ticket to QA column. 
  - **Storage method:** Call HAL API directly (`POST ${baseUrl}/api/artifacts/insert-implementation`) or write to `.hal-tool-call-queue.json`.
  - **Verification:** After storing, call `/api/artifacts/get` to verify all 7 artifacts are present. If any are missing, store them again.
  - **FORBIDDEN:** Claiming "ready" without verifying all artifacts are stored. QA will fail tickets with missing artifacts.

- Artifacts must be **stored in Supabase**:
  - All artifacts must be stored via tool calls before claiming "ready."
  - **Required:** Call the HAL API directly (e.g. `POST ${baseUrl}/api/artifacts/insert-implementation`) so artifacts appear immediately. The `.hal/api-base-url` file must be present. See `.cursor/rules/agent-supabase-api-paradigm.mdc`.
  - Artifacts are linked to tickets via `ticket_pk` in the `agent_artifacts` table.

## PM Review is required before "ready"

- Before claiming "ready for verification," the agent must ensure:
  - PM Review artifact exists in Supabase (stored via `POST /api/artifacts/insert-implementation`),
  - and it reflects the actual implementation state (no placeholders).

## Commit message linkage (no commit hashes in docs)

- Every commit that belongs to a ticket must include the ticket ID in the commit subject (e.g. `feat(0010): ...`, `fix(0008): ...`, `docs(0009): ...`).
- This includes "supporting" commits that are still part of delivering the ticket (examples: `chore`, `docs(rules)`, submodule pointer updates, dependency updates).
- If a commit is **not** part of the ticket, it must be:
  - separated (not bundled into the ticket's commits), and
  - documented in the ticket's Decisions artifact under **Unrequested changes (required)** if it was unavoidable.
 
Note: Workspace-wide, the definition of "done" for **all agents** is enforced by `.cursor/rules/done-means-pushed.mdc`.

## In-App Trace Requirement

- Any meaningful state change performed by the app should be visible in an **in-app debug/diagnostics UI** (not the console).
- Errors should have an in-app representation suitable for a non-technical verifier.

## Scope

- **All artifacts are stored in Supabase**, not in `docs/audit/` folders.
- **FORBIDDEN:** Writing artifact files to `docs/audit/` directories. Use tool calls to store in Supabase.
- Keep each artifact concise; prefer bullets over prose.

## PM Review Artifact (after implementation completes)

- Store PM Review artifact in Supabase via tool call `insert_implementation_artifact` with `artifactType: "pm-review"`.
- Use format from `docs/templates/pm-review.template.md` as reference.
- The PM review must include:
  - **likelihood of success** (0–100%)
  - a ranked list of **potential failures** and how to diagnose them using **in-app** diagnostics

## HAL Tool Call Contract

All artifact and ticket operations are sent to HAL as **tool calls** in your messages. HAL parses and executes them automatically.

**Available tools:**

- **`insert_implementation_artifact`** — Store implementation artifact
  - Params: `{ ticketId: string, artifactType: string, title: string, body_md: string }`

- **`insert_qa_artifact`** — Store QA artifact
  - Params: `{ ticketId: string, title: string, body_md: string }`

- **`get_ticket_content`** — Fetch ticket content
  - Params: `{ ticketId: string }`

- **`update_ticket_body`** — Update ticket body
  - Params: `{ ticketId: string, body_md: string }`

- **`move_ticket_column`** — Move ticket to different column
  - Params: `{ ticketId: string, columnId: string }`

**How to use:** Include tool calls as JSON blocks in your messages. HAL will parse and execute them. You don't need API URLs or credentials.

**Example:**
```
I've completed the plan. Here's my tool call:

{
  "tool": "insert_implementation_artifact",
  "params": {
    "ticketId": "0076",
    "artifactType": "plan",
    "title": "Plan for ticket 0076",
    "body_md": "# Plan\n\n..."
  }
}
```

See `.cursor/rules/agent-supabase-api-paradigm.mdc` for full documentation.
