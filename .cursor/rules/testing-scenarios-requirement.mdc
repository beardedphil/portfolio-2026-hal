---
description: Requirement for agents to document testing scenarios when verifying acceptance criteria
alwaysApply: true
---

# Testing Scenarios Requirement

**MANDATORY:** When any agent (Implementation, QA, or PM/Process Review) marks acceptance criteria as verified/passed, they **must** include a clearly labeled **"Testing scenarios used"** section documenting the specific scenarios they actually ran.

## Where Testing Scenarios Must Appear

The testing scenarios section must appear in the following artifacts:

- **Implementation agent:** In the **Verification** artifact (`artifactType: "verification"`, title: `Verification for ticket <ticket-id>`)
  - **Storage:** Stored in Supabase via HAL API endpoint `/api/artifacts/insert-implementation`
- **QA agent:** In the **QA Report** (`artifactType: "qa-report"`, title: `QA report for ticket <ticket-id>`)
  - **Storage:** Stored in Supabase via HAL API endpoint `/api/artifacts/insert-qa`
- **PM/Process review agent:** In the review artifact/notes when applicable
  - **Storage:** Stored in Supabase via HAL API endpoint `/api/artifacts/insert-implementation` (for PM review artifacts)

## Minimum Content Requirements

The "Testing scenarios used" section must include:

1. **At least 1 happy-path scenario** — The primary success case that validates the core functionality
2. **At least 2 edge/negative scenarios** — Relevant edge cases, error conditions, or boundary conditions that test robustness
3. **Concrete details** — Each scenario must be 1–3 bullets and reference:
   - The specific UI state/inputs used (e.g., "Clicked 'Save' button with empty form", "Entered invalid email format 'test@'")
   - The expected outcome
   - The actual result (PASS/FAIL)

**Do NOT use vague descriptions** like "tested it works" or "verified functionality". Each scenario must be specific and reproducible.

## Example Format

```markdown
## Testing scenarios used

### Happy path
- **Scenario:** User submits form with valid data (name: "John Doe", email: "john@example.com")
  - **Action:** Filled all required fields and clicked "Submit"
  - **Result:** PASS — Form submitted successfully, success message displayed

### Edge cases
- **Scenario:** User submits form with empty required fields
  - **Action:** Left "Name" field empty, clicked "Submit"
  - **Result:** PASS — Validation error displayed, form not submitted

- **Scenario:** User enters email with invalid format
  - **Action:** Entered "test@" in email field, clicked "Submit"
  - **Result:** PASS — Email validation error displayed, form not submitted
```

## When to Include

- **Implementation agent:** Include in Verification artifact when marking acceptance criteria as verified
- **QA agent:** Include in QA Report when performing verification and marking acceptance criteria as passed
- **PM/Process review:** Include in review notes when reviewing implementation and verifying acceptance criteria

## Scope

This requirement applies to all ticketed tasks where acceptance criteria are verified. The testing scenarios section provides transparency and traceability for how acceptance criteria were validated. The scenarios section must be clearly labeled as **"Testing scenarios used"** and scenarios must correspond to the ticket's acceptance criteria. If a ticket has no testable acceptance criteria, state that explicitly in the scenarios section.
